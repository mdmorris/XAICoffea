{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/schubert/.conda/envs/LRP/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/schubert/.conda/envs/LRP/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/schubert/.conda/envs/LRP/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/schubert/.conda/envs/LRP/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/schubert/.conda/envs/LRP/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/schubert/.conda/envs/LRP/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "import keras.backend\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "import keras.utils\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "import sklearn\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_CNN():\n",
    "    input2 = [layers.Input(shape = (len(X_train[i][0]),)) for i in range(1,len(X_train))]\n",
    "    input1 = layers.Input(shape = (grid, grid,1))\n",
    "    x = layers.Conv2D(32, (5, 5), activation = 'relu', padding = 'same')(input1)\n",
    "    x = layers.Conv2D(32, (3, 3), activation = 'relu', padding = 'same')(x)\n",
    "    x = layers.Conv2D(32, (2, 2), activation = 'relu', padding = 'same')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.MaxPool2D((2, 2))(x)\n",
    "    x1 = layers.Flatten()(x)\n",
    "    if input2 == []:\n",
    "        x = layers.Dense(64, activation='relu', name = 'dense1')(x1)\n",
    "    else:\n",
    "        x = layers.concatenate(inputs = [x1] + input2, axis = -1)\n",
    "        x = layers.Dense(64, activation='relu', name = 'dense2')(x)\n",
    "    x = layers.Dense(128, activation='relu', name = 'dense3')(x)\n",
    "    output = layers.Dense(2, activation='softmax')(x)\n",
    "    model = models.Model(inputs= [input1] + input2,\n",
    "                         outputs = output)\n",
    "    opt = keras.optimizers.Adam(lr = 0.001,\n",
    "                                beta_1 = 0.9,\n",
    "                                beta_2 = 0.999,\n",
    "                                amsgrad = False)\n",
    "    model.compile(loss = 'binary_crossentropy',\n",
    "                optimizer = opt,\n",
    "                metrics = ['binary_crossentropy', 'accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_DNN():\n",
    "    input2 = [layers.Input(shape = (len(X_train[i][0]),)) for i in range(0,len(X_train))]\n",
    "    if len(input2) == 1:\n",
    "        x = layers.Dense(64,activation='relu')(input2[0])\n",
    "    else:\n",
    "        x = layers.concatenate(inputs = input2, axis = -1)\n",
    "        x = layers.Dense(64, activation = 'relu')(x)\n",
    "    x = layers.Dense(128, activation = 'relu')(x)\n",
    "    output = layers.Dense(2, activation = 'softmax')(x)\n",
    "    model = models.Model(inputs = input2, \n",
    "                         outputs = output)\n",
    "    opt=keras.optimizers.Adam(lr = 0.0005,\n",
    "                              beta_1 = 0.9,\n",
    "                              beta_2 = 0.9,\n",
    "                              amsgrad = False)\n",
    "    model.compile(loss = 'binary_crossentropy',\n",
    "                optimizer = opt,\n",
    "                metrics = ['binary_crossentropy', 'accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_XY(features, label, dic1, dic2):\n",
    "    X = [np.concatenate((dic1[key], dic2[key])) for key in features]\n",
    "    Y = [np.concatenate((dic1[key], dic2[key])) for key in label]\n",
    "    dim = [ele.shape+(1,) for ele in X]\n",
    "    for i in range(0,len(features)):\n",
    "        X[i] = X[i].reshape(dim[i])\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data:\n",
    "Using Train and Test set produced by preprocessing notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = 16\n",
    "train_sig = np.load('../data/ShowJets_train_ZZ.npz')\n",
    "train_bkg = np.load('../data/ShowJets_train_QCD.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276147\n"
     ]
    }
   ],
   "source": [
    "n_train = len(train_sig['jetPt'])\n",
    "print(n_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "removeFeat = ['jetconstPt_log',\n",
    " 'jetconstEta_abs',\n",
    " 'jetconstE_log',\n",
    " 'jetconstPt_Jetlog',\n",
    " 'charge',\n",
    " 'isEle',\n",
    " 'isPho',\n",
    " 'isMuon',\n",
    " 'isCh',\n",
    " 'isNh',\n",
    " 'delta_eta',\n",
    " 'delta_phi',\n",
    " 'deltaR_jet',\n",
    " 'deltaR_subjet0',\n",
    " 'deltaR_subjet1',\n",
    " 'dxy',\n",
    " 'dz',\n",
    " 'jetpull',\n",
    " 'labels']\n",
    "feat_all = [key for key in train_sig.keys()];\n",
    "for feat in removeFeat:\n",
    "    feat_all.remove(feat)\n",
    "# Swap jetImages into first place for automatized Model building\n",
    "ind_image = feat_all.index('jetImages')\n",
    "feat_all[0], feat_all[ind_image] = feat_all[ind_image], feat_all[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jetImages',\n",
       " 'jetMassSD',\n",
       " 'deltaR_subjets',\n",
       " 'jetPt',\n",
       " 'z',\n",
       " 'tau1_b05',\n",
       " 'tau2_b05',\n",
       " 'tau3_b05',\n",
       " 'tau1_sd_b05',\n",
       " 'tau2_sd_b05',\n",
       " 'tau3_sd_b05',\n",
       " 'tau1_b10',\n",
       " 'tau2_b10',\n",
       " 'tau3_b10',\n",
       " 'tau1_sd_b10',\n",
       " 'tau2_sd_b10',\n",
       " 'tau3_sd_b10',\n",
       " 'tau1_b15',\n",
       " 'tau2_b15',\n",
       " 'tau3_b15',\n",
       " 'tau1_sd_b15',\n",
       " 'tau2_sd_b15',\n",
       " 'tau3_sd_b15',\n",
       " 'tau1_b20',\n",
       " 'tau2_b20',\n",
       " 'tau3_b20',\n",
       " 'tau1_sd_b20',\n",
       " 'tau2_sd_b20',\n",
       " 'tau3_sd_b20',\n",
       " 'jetMass',\n",
       " 'jetEta',\n",
       " 'jetPhi',\n",
       " 'chMult',\n",
       " 'neutMult',\n",
       " 'phoMult',\n",
       " 'eleMult',\n",
       " 'muMult',\n",
       " 'beta3',\n",
       " 'beta3_sd',\n",
       " 'tau21',\n",
       " 'jetpull_abs',\n",
       " 'dxy_max',\n",
       " 'dz_max']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model with only Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features will be loaded into X\n",
    "features = ['jetImages']\n",
    "\n",
    "# label into Y\n",
    "label = ['labels']\n",
    "X_train, Y_train = build_XY(features, label, train_sig, train_bkg)\n",
    "Y_train = [Y_train[0][:,:2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN = build_CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 5ms/step\n",
      "[[0.49998346 0.50001657]\n",
      " [0.49940646 0.50059354]\n",
      " [0.4996836  0.50031644]\n",
      " [0.49941456 0.50058544]\n",
      " [0.4998332  0.50016683]\n",
      " [0.49981147 0.50018847]\n",
      " [0.4996562  0.50034374]\n",
      " [0.4996473  0.50035274]\n",
      " [0.49946055 0.5005394 ]\n",
      " [0.49941832 0.5005816 ]]\n",
      "[0.6924105882644653, 0.6924105882644653, 1.0]\n"
     ]
    }
   ],
   "source": [
    "X_batch = [ele[:10] for ele in X_train]\n",
    "Y_batch = [ele[:10] for ele in Y_train]\n",
    "example_result = CNN.predict(x = X_batch)\n",
    "results = CNN.evaluate(x = X_batch, y = Y_batch )\n",
    "print(example_result)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 441835 samples, validate on 110459 samples\n",
      "Epoch 1/80\n",
      "441835/441835 [==============================] - 508s 1ms/step - loss: 0.5639 - binary_crossentropy: 0.5639 - acc: 0.6990 - val_loss: 0.7213 - val_binary_crossentropy: 0.7213 - val_acc: 0.6361\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.72127, saving model to model/CNN_Images.h1\n",
      "Epoch 2/80\n",
      " 55744/441835 [==>...........................] - ETA: 6:06 - loss: 0.5459 - binary_crossentropy: 0.5459 - acc: 0.7112"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"model/CNN_Images.h1\"\n",
    "if not os.path.exists(\"model\"):\n",
    "    os.mkdir(\"model\")\n",
    "\n",
    "model_checkpoint = keras.callbacks.ModelCheckpoint(checkpoint_path, monitor = 'val_loss', \n",
    "                                                   verbose = 1, save_best_only = True, \n",
    "                                                   save_weights_only = False, mode = 'auto', \n",
    "                                                   period = 1)    \n",
    "EPOCHS = 80\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 10)\n",
    "history = CNN.fit(\n",
    "    X_train, Y_train,\n",
    "    epochs = EPOCHS, \n",
    "    validation_split = 0.2,\n",
    "    verbose = 1,\n",
    "    callbacks = [early_stop, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('history/CNN_Images.history', 'wb') as file_pi:\n",
    "    pickle.dump(history.history, file_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model with all Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Features for Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all possible features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features will be loaded into X\n",
    "features = feat_all\n",
    "\n",
    "# label into Y\n",
    "label = ['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = build_XY(features, label, train_sig, train_bkg)\n",
    "Y_train = [Y_train[0][:,:2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN = build_CNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test model's prediction $before$ training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 8ms/step\n",
      "[[0.5229908  0.47700912]\n",
      " [0.50503963 0.4949604 ]\n",
      " [0.52277404 0.477226  ]\n",
      " [0.51378024 0.4862197 ]\n",
      " [0.5261511  0.4738489 ]\n",
      " [0.51986927 0.48013073]\n",
      " [0.50343287 0.4965672 ]\n",
      " [0.50274724 0.4972528 ]\n",
      " [0.5291937  0.47080636]\n",
      " [0.513993   0.486007  ]]\n",
      "[0.725845456123352, 0.725845456123352, 0.0]\n"
     ]
    }
   ],
   "source": [
    "X_batch = [ele[:10] for ele in X_train]\n",
    "Y_batch = [ele[:10] for ele in Y_train]\n",
    "example_result = CNN.predict(x = X_batch)\n",
    "results = CNN.evaluate(x = X_batch, y = Y_batch )\n",
    "print(example_result)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train! (warning: if building CNN, computer tends to get loud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1342291 samples, validate on 335573 samples\n",
      "Epoch 1/60\n",
      "1342291/1342291 [==============================] - 1580s 1ms/step - loss: 0.3542 - binary_crossentropy: 0.3542 - acc: 0.8446 - val_loss: 0.3314 - val_binary_crossentropy: 0.3314 - val_acc: 0.8557\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.33144, saving model to model/CNN_all.h8\n",
      "Epoch 2/60\n",
      "1342291/1342291 [==============================] - 1778s 1ms/step - loss: 0.3320 - binary_crossentropy: 0.3320 - acc: 0.8557 - val_loss: 0.3233 - val_binary_crossentropy: 0.3233 - val_acc: 0.8604\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.33144 to 0.32329, saving model to model/CNN_all.h8\n",
      "Epoch 3/60\n",
      "1342291/1342291 [==============================] - 1719s 1ms/step - loss: 0.3265 - binary_crossentropy: 0.3265 - acc: 0.8583 - val_loss: 0.3290 - val_binary_crossentropy: 0.3290 - val_acc: 0.8565\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.32329\n",
      "Epoch 4/60\n",
      "1342291/1342291 [==============================] - 1842s 1ms/step - loss: 0.3231 - binary_crossentropy: 0.3231 - acc: 0.8602 - val_loss: 0.3221 - val_binary_crossentropy: 0.3221 - val_acc: 0.8605\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.32329 to 0.32209, saving model to model/CNN_all.h8\n",
      "Epoch 5/60\n",
      "1342291/1342291 [==============================] - 1595s 1ms/step - loss: 0.3210 - binary_crossentropy: 0.3210 - acc: 0.8611 - val_loss: 0.3150 - val_binary_crossentropy: 0.3150 - val_acc: 0.8637\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.32209 to 0.31501, saving model to model/CNN_all.h8\n",
      "Epoch 6/60\n",
      "1342291/1342291 [==============================] - 1418s 1ms/step - loss: 0.3191 - binary_crossentropy: 0.3191 - acc: 0.8620 - val_loss: 0.3204 - val_binary_crossentropy: 0.3204 - val_acc: 0.8621\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.31501\n",
      "Epoch 7/60\n",
      "1342291/1342291 [==============================] - 1705s 1ms/step - loss: 0.3179 - binary_crossentropy: 0.3179 - acc: 0.8626 - val_loss: 0.3166 - val_binary_crossentropy: 0.3166 - val_acc: 0.8628\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.31501\n",
      "Epoch 8/60\n",
      "1342291/1342291 [==============================] - 1506s 1ms/step - loss: 0.3169 - binary_crossentropy: 0.3169 - acc: 0.8629 - val_loss: 0.3144 - val_binary_crossentropy: 0.3144 - val_acc: 0.8643\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.31501 to 0.31441, saving model to model/CNN_all.h8\n",
      "Epoch 9/60\n",
      "1342291/1342291 [==============================] - 1575s 1ms/step - loss: 0.3158 - binary_crossentropy: 0.3158 - acc: 0.8634 - val_loss: 0.3195 - val_binary_crossentropy: 0.3195 - val_acc: 0.8611\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.31441\n",
      "Epoch 10/60\n",
      "1342291/1342291 [==============================] - 1553s 1ms/step - loss: 0.3152 - binary_crossentropy: 0.3152 - acc: 0.8638 - val_loss: 0.3143 - val_binary_crossentropy: 0.3143 - val_acc: 0.8640\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.31441 to 0.31427, saving model to model/CNN_all.h8\n",
      "Epoch 11/60\n",
      "1342291/1342291 [==============================] - 2027s 2ms/step - loss: 0.3145 - binary_crossentropy: 0.3145 - acc: 0.8641 - val_loss: 0.3132 - val_binary_crossentropy: 0.3132 - val_acc: 0.8646\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.31427 to 0.31321, saving model to model/CNN_all.h8\n",
      "Epoch 12/60\n",
      "1342291/1342291 [==============================] - 1903s 1ms/step - loss: 0.3139 - binary_crossentropy: 0.3139 - acc: 0.8644 - val_loss: 0.3139 - val_binary_crossentropy: 0.3139 - val_acc: 0.8651\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.31321\n",
      "Epoch 13/60\n",
      "1342291/1342291 [==============================] - 1687s 1ms/step - loss: 0.3136 - binary_crossentropy: 0.3136 - acc: 0.8646 - val_loss: 0.3119 - val_binary_crossentropy: 0.3119 - val_acc: 0.8655\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.31321 to 0.31189, saving model to model/CNN_all.h8\n",
      "Epoch 14/60\n",
      "1342291/1342291 [==============================] - 1663s 1ms/step - loss: 0.3132 - binary_crossentropy: 0.3132 - acc: 0.8650 - val_loss: 0.3126 - val_binary_crossentropy: 0.3126 - val_acc: 0.8648\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.31189\n",
      "Epoch 15/60\n",
      "1342291/1342291 [==============================] - 1760s 1ms/step - loss: 0.3127 - binary_crossentropy: 0.3127 - acc: 0.8650 - val_loss: 0.3110 - val_binary_crossentropy: 0.3110 - val_acc: 0.8659\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.31189 to 0.31102, saving model to model/CNN_all.h8\n",
      "Epoch 16/60\n",
      "1342291/1342291 [==============================] - 1570s 1ms/step - loss: 0.3123 - binary_crossentropy: 0.3123 - acc: 0.8653 - val_loss: 0.3091 - val_binary_crossentropy: 0.3091 - val_acc: 0.8666\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.31102 to 0.30914, saving model to model/CNN_all.h8\n",
      "Epoch 17/60\n",
      "1342291/1342291 [==============================] - 1782s 1ms/step - loss: 0.3121 - binary_crossentropy: 0.3121 - acc: 0.8655 - val_loss: 0.3095 - val_binary_crossentropy: 0.3095 - val_acc: 0.8661\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.30914\n",
      "Epoch 18/60\n",
      "1342291/1342291 [==============================] - 65839s 49ms/step - loss: 0.3117 - binary_crossentropy: 0.3117 - acc: 0.8655 - val_loss: 0.3124 - val_binary_crossentropy: 0.3124 - val_acc: 0.8644\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.30914\n",
      "Epoch 19/60\n",
      "1342291/1342291 [==============================] - 1713s 1ms/step - loss: 0.3114 - binary_crossentropy: 0.3114 - acc: 0.8658 - val_loss: 0.3160 - val_binary_crossentropy: 0.3160 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.30914\n",
      "Epoch 20/60\n",
      "1342291/1342291 [==============================] - 1756s 1ms/step - loss: 0.3111 - binary_crossentropy: 0.3111 - acc: 0.8660 - val_loss: 0.3113 - val_binary_crossentropy: 0.3113 - val_acc: 0.8650\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.30914\n",
      "Epoch 21/60\n",
      "1342291/1342291 [==============================] - 1702s 1ms/step - loss: 0.3107 - binary_crossentropy: 0.3107 - acc: 0.8661 - val_loss: 0.3137 - val_binary_crossentropy: 0.3137 - val_acc: 0.8646\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.30914\n",
      "Epoch 22/60\n",
      "1342291/1342291 [==============================] - 1623s 1ms/step - loss: 0.3106 - binary_crossentropy: 0.3106 - acc: 0.8661 - val_loss: 0.3080 - val_binary_crossentropy: 0.3080 - val_acc: 0.8669\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.30914 to 0.30797, saving model to model/CNN_all.h8\n",
      "Epoch 23/60\n",
      "1342291/1342291 [==============================] - 1639s 1ms/step - loss: 0.3102 - binary_crossentropy: 0.3102 - acc: 0.8662 - val_loss: 0.3076 - val_binary_crossentropy: 0.3076 - val_acc: 0.8668\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.30797 to 0.30763, saving model to model/CNN_all.h8\n",
      "Epoch 24/60\n",
      "1342291/1342291 [==============================] - 1674s 1ms/step - loss: 0.3102 - binary_crossentropy: 0.3102 - acc: 0.8665 - val_loss: 0.3158 - val_binary_crossentropy: 0.3158 - val_acc: 0.8644\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.30763\n",
      "Epoch 25/60\n",
      "1342291/1342291 [==============================] - 1699s 1ms/step - loss: 0.3097 - binary_crossentropy: 0.3097 - acc: 0.8664 - val_loss: 0.3113 - val_binary_crossentropy: 0.3113 - val_acc: 0.8665\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.30763\n",
      "Epoch 26/60\n",
      "1342291/1342291 [==============================] - 1729s 1ms/step - loss: 0.3097 - binary_crossentropy: 0.3097 - acc: 0.8664 - val_loss: 0.3080 - val_binary_crossentropy: 0.3080 - val_acc: 0.8673\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.30763\n",
      "Epoch 27/60\n",
      "1342291/1342291 [==============================] - 1744s 1ms/step - loss: 0.3094 - binary_crossentropy: 0.3094 - acc: 0.8669 - val_loss: 0.3130 - val_binary_crossentropy: 0.3130 - val_acc: 0.8657\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.30763\n",
      "Epoch 28/60\n",
      "1342291/1342291 [==============================] - 1757s 1ms/step - loss: 0.3092 - binary_crossentropy: 0.3092 - acc: 0.8667 - val_loss: 0.3159 - val_binary_crossentropy: 0.3159 - val_acc: 0.8670\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.30763\n",
      "Epoch 29/60\n",
      "1342291/1342291 [==============================] - 1146s 854us/step - loss: 0.3091 - binary_crossentropy: 0.3091 - acc: 0.8668 - val_loss: 0.3089 - val_binary_crossentropy: 0.3089 - val_acc: 0.8664\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.30763\n",
      "Epoch 30/60\n",
      "1342291/1342291 [==============================] - 800s 596us/step - loss: 0.3089 - binary_crossentropy: 0.3089 - acc: 0.8669 - val_loss: 0.3082 - val_binary_crossentropy: 0.3082 - val_acc: 0.8668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00030: val_loss did not improve from 0.30763\n",
      "Epoch 31/60\n",
      "1342291/1342291 [==============================] - 799s 595us/step - loss: 0.3089 - binary_crossentropy: 0.3089 - acc: 0.8670 - val_loss: 0.3072 - val_binary_crossentropy: 0.3072 - val_acc: 0.8673\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.30763 to 0.30722, saving model to model/CNN_all.h8\n",
      "Epoch 32/60\n",
      "1342291/1342291 [==============================] - 798s 595us/step - loss: 0.3085 - binary_crossentropy: 0.3085 - acc: 0.8671 - val_loss: 0.3076 - val_binary_crossentropy: 0.3076 - val_acc: 0.8671\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.30722\n",
      "Epoch 33/60\n",
      "1342291/1342291 [==============================] - 799s 595us/step - loss: 0.3084 - binary_crossentropy: 0.3084 - acc: 0.8671 - val_loss: 0.3081 - val_binary_crossentropy: 0.3081 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.30722\n",
      "Epoch 34/60\n",
      "1342291/1342291 [==============================] - 799s 595us/step - loss: 0.3083 - binary_crossentropy: 0.3083 - acc: 0.8673 - val_loss: 0.3077 - val_binary_crossentropy: 0.3077 - val_acc: 0.8676\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.30722\n",
      "Epoch 35/60\n",
      "1342291/1342291 [==============================] - 798s 594us/step - loss: 0.3081 - binary_crossentropy: 0.3081 - acc: 0.8672 - val_loss: 0.3069 - val_binary_crossentropy: 0.3069 - val_acc: 0.8671\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.30722 to 0.30687, saving model to model/CNN_all.h8\n",
      "Epoch 36/60\n",
      "1342291/1342291 [==============================] - 800s 596us/step - loss: 0.3081 - binary_crossentropy: 0.3081 - acc: 0.8672 - val_loss: 0.3099 - val_binary_crossentropy: 0.3099 - val_acc: 0.8664\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.30687\n",
      "Epoch 37/60\n",
      "1342291/1342291 [==============================] - 798s 595us/step - loss: 0.3080 - binary_crossentropy: 0.3080 - acc: 0.8674 - val_loss: 0.3089 - val_binary_crossentropy: 0.3089 - val_acc: 0.8672\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.30687\n",
      "Epoch 38/60\n",
      "1342291/1342291 [==============================] - 799s 596us/step - loss: 0.3077 - binary_crossentropy: 0.3077 - acc: 0.8676 - val_loss: 0.3083 - val_binary_crossentropy: 0.3083 - val_acc: 0.8669\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.30687\n",
      "Epoch 39/60\n",
      "1342291/1342291 [==============================] - 799s 595us/step - loss: 0.3078 - binary_crossentropy: 0.3078 - acc: 0.8676 - val_loss: 0.3114 - val_binary_crossentropy: 0.3114 - val_acc: 0.8654\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.30687\n",
      "Epoch 40/60\n",
      "1342291/1342291 [==============================] - 1240s 924us/step - loss: 0.3076 - binary_crossentropy: 0.3076 - acc: 0.8676 - val_loss: 0.3074 - val_binary_crossentropy: 0.3074 - val_acc: 0.8681\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.30687\n",
      "Epoch 41/60\n",
      "1342291/1342291 [==============================] - 1603s 1ms/step - loss: 0.3074 - binary_crossentropy: 0.3074 - acc: 0.8677 - val_loss: 0.3094 - val_binary_crossentropy: 0.3094 - val_acc: 0.8669\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.30687\n",
      "Epoch 42/60\n",
      "1342291/1342291 [==============================] - 1690s 1ms/step - loss: 0.3073 - binary_crossentropy: 0.3073 - acc: 0.8678 - val_loss: 0.3069 - val_binary_crossentropy: 0.3069 - val_acc: 0.8678\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.30687\n",
      "Epoch 43/60\n",
      "1342291/1342291 [==============================] - 1720s 1ms/step - loss: 0.3073 - binary_crossentropy: 0.3073 - acc: 0.8677 - val_loss: 0.3101 - val_binary_crossentropy: 0.3101 - val_acc: 0.8672\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.30687\n",
      "Epoch 44/60\n",
      "1342291/1342291 [==============================] - 1731s 1ms/step - loss: 0.3072 - binary_crossentropy: 0.3072 - acc: 0.8675 - val_loss: 0.3070 - val_binary_crossentropy: 0.3070 - val_acc: 0.8675\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.30687\n",
      "Epoch 45/60\n",
      "1342291/1342291 [==============================] - 1781s 1ms/step - loss: 0.3072 - binary_crossentropy: 0.3072 - acc: 0.8678 - val_loss: 0.3094 - val_binary_crossentropy: 0.3094 - val_acc: 0.8664\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.30687\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"model/CNN_all.h8\"\n",
    "if not os.path.exists(\"model\"):\n",
    "    os.mkdir(\"model\")\n",
    "\n",
    "model_checkpoint = keras.callbacks.ModelCheckpoint(checkpoint_path, monitor = 'val_loss', \n",
    "                                                   verbose = 1, save_best_only = True, \n",
    "                                                   save_weights_only = False, mode = 'auto', \n",
    "                                                   period = 1)    \n",
    "EPOCHS = 80\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 10)\n",
    "history = CNN.fit(\n",
    "    X_train, Y_train,\n",
    "    epochs = EPOCHS,\n",
    "    validation_split = 0.2,\n",
    "    verbose = 1,\n",
    "    callbacks = [early_stop, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('history/CNN_all.h8.history', 'wb') as file_pi:\n",
    "    pickle.dump(history.history, file_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Features for Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all possible features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jetImages',\n",
       " 'jetPt',\n",
       " 'jetEta',\n",
       " 'jetPhi',\n",
       " 'jetMass',\n",
       " 'jetMassSD',\n",
       " 'tau1_b05',\n",
       " 'tau2_b05',\n",
       " 'tau3_b05',\n",
       " 'tau1_sd_b05',\n",
       " 'tau2_sd_b05',\n",
       " 'tau3_sd_b05',\n",
       " 'tau1_b10',\n",
       " 'tau2_b10',\n",
       " 'tau3_b10',\n",
       " 'tau1_sd_b10',\n",
       " 'tau2_sd_b10',\n",
       " 'tau3_sd_b10',\n",
       " 'tau1_b20',\n",
       " 'tau2_b20',\n",
       " 'tau3_b20',\n",
       " 'tau1_sd_b20',\n",
       " 'tau2_sd_b20',\n",
       " 'tau3_sd_b20',\n",
       " 'chMult',\n",
       " 'neutMult',\n",
       " 'phoMult',\n",
       " 'eleMult',\n",
       " 'muMult',\n",
       " 'jetpull',\n",
       " 'beta3',\n",
       " 'beta3_sd',\n",
       " 'tau21']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_those = [ 'jetImages',\n",
    " 'chMult',\n",
    " 'neutMult',\n",
    " 'phoMult',\n",
    " 'eleMult',\n",
    " 'muMult']\n",
    "# features will be loaded into X\n",
    "features = feat_all[:]\n",
    "for feat in drop_those:\n",
    "    features.remove(feat)\n",
    "# label into Y\n",
    "label = ['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jetPt',\n",
       " 'jetEta',\n",
       " 'jetPhi',\n",
       " 'jetMass',\n",
       " 'jetMassSD',\n",
       " 'tau1_b05',\n",
       " 'tau2_b05',\n",
       " 'tau3_b05',\n",
       " 'tau1_sd_b05',\n",
       " 'tau2_sd_b05',\n",
       " 'tau3_sd_b05',\n",
       " 'tau1_b10',\n",
       " 'tau2_b10',\n",
       " 'tau3_b10',\n",
       " 'tau1_sd_b10',\n",
       " 'tau2_sd_b10',\n",
       " 'tau3_sd_b10',\n",
       " 'tau1_b20',\n",
       " 'tau2_b20',\n",
       " 'tau3_b20',\n",
       " 'tau1_sd_b20',\n",
       " 'tau2_sd_b20',\n",
       " 'tau3_sd_b20',\n",
       " 'jetpull',\n",
       " 'beta3',\n",
       " 'beta3_sd',\n",
       " 'tau21']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = build_XY(features,label,data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN = build_DNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test model's prediction $before$ training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 10ms/step\n",
      "[[0.5270552  0.4729448 ]\n",
      " [0.5053764  0.4946237 ]\n",
      " [0.5052645  0.49473548]\n",
      " [0.51708007 0.48291987]\n",
      " [0.48523638 0.51476365]\n",
      " [0.54981667 0.45018327]\n",
      " [0.5066523  0.4933477 ]\n",
      " [0.5062908  0.49370918]\n",
      " [0.51445496 0.48554504]\n",
      " [0.5148447  0.48515528]]\n",
      "[0.6866416931152344, 0.6866416931152344, 0.5]\n"
     ]
    }
   ],
   "source": [
    "X_batch = [ele[:10] for ele in X_train]\n",
    "Y_batch = [ele[:10] for ele in Y_train]\n",
    "example_result = CNN.predict(x = X_batch)\n",
    "results = CNN.evaluate(x = X_batch, y = Y_batch )\n",
    "print(example_result)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train! (warning: if building CNN, computer tends to get loud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1342291 samples, validate on 335573 samples\n",
      "Epoch 1/60\n",
      "1342291/1342291 [==============================] - 78s 58us/step - loss: 0.3736 - binary_crossentropy: 0.3736 - acc: 0.8358 - val_loss: 0.3517 - val_binary_crossentropy: 0.3517 - val_acc: 0.8485\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.35172, saving model to model/DNN_noflavour.h1\n",
      "Epoch 2/60\n",
      "1342291/1342291 [==============================] - 81s 60us/step - loss: 0.3526 - binary_crossentropy: 0.3526 - acc: 0.8483 - val_loss: 0.3554 - val_binary_crossentropy: 0.3554 - val_acc: 0.8486\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.35172\n",
      "Epoch 3/60\n",
      "1342291/1342291 [==============================] - 83s 62us/step - loss: 0.3563 - binary_crossentropy: 0.3563 - acc: 0.8490 - val_loss: 0.3508 - val_binary_crossentropy: 0.3508 - val_acc: 0.8504\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.35172 to 0.35081, saving model to model/DNN_noflavour.h1\n",
      "Epoch 4/60\n",
      "1342291/1342291 [==============================] - 83s 62us/step - loss: 0.3575 - binary_crossentropy: 0.3575 - acc: 0.8494 - val_loss: 0.3629 - val_binary_crossentropy: 0.3629 - val_acc: 0.8460\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35081\n",
      "Epoch 5/60\n",
      "1342291/1342291 [==============================] - 84s 63us/step - loss: 0.3575 - binary_crossentropy: 0.3575 - acc: 0.8495 - val_loss: 0.3520 - val_binary_crossentropy: 0.3520 - val_acc: 0.8514\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35081\n",
      "Epoch 6/60\n",
      "1342291/1342291 [==============================] - 83s 62us/step - loss: 0.3557 - binary_crossentropy: 0.3557 - acc: 0.8496 - val_loss: 0.3482 - val_binary_crossentropy: 0.3482 - val_acc: 0.8532\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35081 to 0.34816, saving model to model/DNN_noflavour.h1\n",
      "Epoch 7/60\n",
      "1342291/1342291 [==============================] - 84s 62us/step - loss: 0.3569 - binary_crossentropy: 0.3569 - acc: 0.8499 - val_loss: 0.3552 - val_binary_crossentropy: 0.3552 - val_acc: 0.8520\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.34816\n",
      "Epoch 8/60\n",
      "1342291/1342291 [==============================] - 85s 63us/step - loss: 0.3585 - binary_crossentropy: 0.3585 - acc: 0.8500 - val_loss: 0.3620 - val_binary_crossentropy: 0.3620 - val_acc: 0.8494\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.34816\n",
      "Epoch 9/60\n",
      "1342291/1342291 [==============================] - 84s 63us/step - loss: 0.3609 - binary_crossentropy: 0.3609 - acc: 0.8494 - val_loss: 0.3625 - val_binary_crossentropy: 0.3625 - val_acc: 0.8489\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.34816\n",
      "Epoch 10/60\n",
      "1342291/1342291 [==============================] - 86s 64us/step - loss: 0.3618 - binary_crossentropy: 0.3618 - acc: 0.8496 - val_loss: 0.3577 - val_binary_crossentropy: 0.3577 - val_acc: 0.8488\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.34816\n",
      "Epoch 11/60\n",
      "1342291/1342291 [==============================] - 86s 64us/step - loss: 0.3618 - binary_crossentropy: 0.3618 - acc: 0.8497 - val_loss: 0.3634 - val_binary_crossentropy: 0.3634 - val_acc: 0.8476\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.34816\n",
      "Epoch 12/60\n",
      "1342291/1342291 [==============================] - 87s 65us/step - loss: 0.3639 - binary_crossentropy: 0.3639 - acc: 0.8494 - val_loss: 0.3695 - val_binary_crossentropy: 0.3695 - val_acc: 0.8483\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.34816\n",
      "Epoch 13/60\n",
      "1342291/1342291 [==============================] - 86s 64us/step - loss: 0.3651 - binary_crossentropy: 0.3651 - acc: 0.8495 - val_loss: 0.3585 - val_binary_crossentropy: 0.3585 - val_acc: 0.8520\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.34816\n",
      "Epoch 14/60\n",
      "1342291/1342291 [==============================] - 86s 64us/step - loss: 0.3644 - binary_crossentropy: 0.3644 - acc: 0.8495 - val_loss: 0.3662 - val_binary_crossentropy: 0.3662 - val_acc: 0.8473\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.34816\n",
      "Epoch 15/60\n",
      "1342291/1342291 [==============================] - 86s 64us/step - loss: 0.3670 - binary_crossentropy: 0.3670 - acc: 0.8495 - val_loss: 0.3719 - val_binary_crossentropy: 0.3719 - val_acc: 0.8468\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.34816\n",
      "Epoch 16/60\n",
      "1342291/1342291 [==============================] - 86s 64us/step - loss: 0.3670 - binary_crossentropy: 0.3670 - acc: 0.8497 - val_loss: 0.3631 - val_binary_crossentropy: 0.3631 - val_acc: 0.8516\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.34816\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"model/DNN_noflavour.h1\"\n",
    "if not os.path.exists(\"model\"):\n",
    "    os.mkdir(\"model\")\n",
    "\n",
    "model_checkpoint = keras.callbacks.ModelCheckpoint(checkpoint_path, monitor = 'val_loss', \n",
    "                                                   verbose = 1, save_best_only = True, \n",
    "                                                   save_weights_only = False, mode = 'auto', \n",
    "                                                   period = 1)    \n",
    "EPOCHS = 60\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 10)\n",
    "history = CNN.fit(\n",
    "    X_train, Y_train,\n",
    "    epochs = EPOCHS,\n",
    "    validation_split = 0.2,\n",
    "    verbose = 1,\n",
    "    callbacks = [early_stop, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('history/DNN_noflavour.h1.history', 'wb') as file_pi:\n",
    "    pickle.dump(history.history, file_pi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
