{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "import keras.backend\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "import keras.utils\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "import sklearn\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_CNN():\n",
    "    input2 = [layers.Input(shape = (len(X_train[i][0]),)) for i in range(1,len(X_train))]\n",
    "    input1 = layers.Input(shape = (grid, grid,1))\n",
    "    x = layers.Conv2D(32, (5, 5), activation = 'relu', padding = 'same')(input1)\n",
    "    x = layers.Conv2D(32, (3, 3), activation = 'relu', padding = 'same')(x)\n",
    "    x = layers.Conv2D(32, (2, 2), activation = 'relu', padding = 'same')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.MaxPool2D((2, 2))(x)\n",
    "    x1 = layers.Flatten()(x)\n",
    "    if input2 == []:\n",
    "        x = layers.Dense(64, activation='relu', name = 'dense1')(x1)\n",
    "    else:\n",
    "        x = layers.concatenate(inputs = [x1] + input2, axis = -1)\n",
    "        x = layers.Dense(64, activation='relu', name = 'dense2')(x)\n",
    "    x = layers.Dense(128, activation='relu', name = 'dense3')(x)\n",
    "    output = layers.Dense(2, activation='softmax')(x)\n",
    "    model = models.Model(inputs= [input1] + input2,\n",
    "                         outputs = output)\n",
    "    opt = keras.optimizers.Adam(lr = 0.001,\n",
    "                                beta_1 = 0.9,\n",
    "                                beta_2 = 0.999,\n",
    "                                amsgrad = False)\n",
    "    model.compile(loss = 'binary_crossentropy',\n",
    "                optimizer = opt,\n",
    "                metrics = ['binary_crossentropy', 'accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_DNN():\n",
    "    input2 = [layers.Input(shape = (len(X_train[i][0]),)) for i in range(0,len(X_train))]\n",
    "    if len(input2) == 1:\n",
    "        x = layers.Dense(64,activation='relu')(input2[0])\n",
    "    else:\n",
    "        x = layers.concatenate(inputs = input2, axis = -1)\n",
    "        x = layers.Dense(64, activation = 'relu')(x)\n",
    "    x = layers.Dense(128, activation = 'relu')(x)\n",
    "    output = layers.Dense(2, activation = 'softmax')(x)\n",
    "    model = models.Model(inputs = input2, \n",
    "                         outputs = output)\n",
    "    opt=keras.optimizers.Adam(lr = 0.0005,\n",
    "                              beta_1 = 0.9,\n",
    "                              beta_2 = 0.9,\n",
    "                              amsgrad = False)\n",
    "    model.compile(loss = 'binary_crossentropy',\n",
    "                optimizer = opt,\n",
    "                metrics = ['binary_crossentropy', 'accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_XY(features, label, dic1, dic2):\n",
    "    X = [np.concatenate((dic1[key], dic2[key])) for key in features]\n",
    "    Y = [np.concatenate((dic1[key], dic2[key])) for key in label]\n",
    "    dim = [ele.shape+(1,) for ele in X]\n",
    "    for i in range(0,len(features)):\n",
    "        X[i] = X[i].reshape(dim[i])\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data:\n",
    "Using Train and Test set produced by preprocessing notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = 16\n",
    "train_sig = np.load('/mnt/data/ml/PreProcessing/ShowJets_train_ZZ.npz')\n",
    "train_bkg = np.load('/mnt/data/ml/PreProcessing/ShowJets_train_QCD.npz')\n",
    "#train_sig = np.load('../data/ShowJets_train_ZZ.npz')\n",
    "#train_bkg = np.load('../data/ShowJets_train_QCD.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276147\n"
     ]
    }
   ],
   "source": [
    "n_train = len(train_sig['jetPt'])\n",
    "print(n_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "removeFeat = ['jetconstPt_log',\n",
    " 'jetconstEta_abs',\n",
    " 'jetconstE_log',\n",
    " 'jetconstPt_Jetlog',\n",
    " 'charge',\n",
    " 'isEle',\n",
    " 'isPho',\n",
    " 'isMuon',\n",
    " 'isCh',\n",
    " 'isNh',\n",
    " 'delta_eta',\n",
    " 'delta_phi',\n",
    " 'deltaR_jet',\n",
    " 'deltaR_subjet0',\n",
    " 'deltaR_subjet1',\n",
    " 'dxy',\n",
    " 'dz',\n",
    " 'jetpull',\n",
    " 'labels']\n",
    "feat_all = [key for key in train_sig.keys()];\n",
    "for feat in removeFeat:\n",
    "    feat_all.remove(feat)\n",
    "# Swap jetImages into first place for automatized Model building\n",
    "ind_image = feat_all.index('jetImages')\n",
    "feat_all[0], feat_all[ind_image] = feat_all[ind_image], feat_all[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jetImages',\n",
       " 'jetMassSD',\n",
       " 'deltaR_subjets',\n",
       " 'jetPt',\n",
       " 'z',\n",
       " 'jetMass',\n",
       " 'jetEta',\n",
       " 'jetPhi',\n",
       " 'tau1_b05',\n",
       " 'tau2_b05',\n",
       " 'tau3_b05',\n",
       " 'tau1_sd_b05',\n",
       " 'tau2_sd_b05',\n",
       " 'tau3_sd_b05',\n",
       " 'tau1_b10',\n",
       " 'tau2_b10',\n",
       " 'tau3_b10',\n",
       " 'tau1_sd_b10',\n",
       " 'tau2_sd_b10',\n",
       " 'tau3_sd_b10',\n",
       " 'tau1_b15',\n",
       " 'tau2_b15',\n",
       " 'tau3_b15',\n",
       " 'tau1_sd_b15',\n",
       " 'tau2_sd_b15',\n",
       " 'tau3_sd_b15',\n",
       " 'tau1_b20',\n",
       " 'tau2_b20',\n",
       " 'tau3_b20',\n",
       " 'tau1_sd_b20',\n",
       " 'tau2_sd_b20',\n",
       " 'tau3_sd_b20',\n",
       " 'chMult',\n",
       " 'neutMult',\n",
       " 'phoMult',\n",
       " 'eleMult',\n",
       " 'muMult',\n",
       " 'beta3',\n",
       " 'beta3_sd',\n",
       " 'tau21',\n",
       " 'jetpull_abs',\n",
       " 'dxy_max',\n",
       " 'dz_max']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model with only Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features will be loaded into X\n",
    "features = ['jetImages']\n",
    "\n",
    "# label into Y\n",
    "label = ['labels']\n",
    "X_train, Y_train = build_XY(features, label, train_sig, train_bkg)\n",
    "Y_train = [Y_train[0][:,:2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "CNN = build_CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 4ms/step\n",
      "[[0.50025594 0.49974406]\n",
      " [0.49959326 0.50040674]\n",
      " [0.49986622 0.5001338 ]\n",
      " [0.4997814  0.50021863]\n",
      " [0.50004095 0.49995908]\n",
      " [0.50019866 0.4998013 ]\n",
      " [0.500176   0.49982402]\n",
      " [0.5000398  0.49996015]\n",
      " [0.49949938 0.50050056]\n",
      " [0.49961916 0.5003809 ]]\n",
      "[0.6929615139961243, 0.6929615139961243, 0.5]\n"
     ]
    }
   ],
   "source": [
    "X_batch = [ele[:10] for ele in X_train]\n",
    "Y_batch = [ele[:10] for ele in Y_train]\n",
    "example_result = CNN.predict(x = X_batch)\n",
    "results = CNN.evaluate(x = X_batch, y = Y_batch )\n",
    "print(example_result)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 441835 samples, validate on 110459 samples\n",
      "Epoch 1/80\n",
      "441835/441835 [==============================] - 240s 544us/step - loss: 0.5579 - binary_crossentropy: 0.5579 - acc: 0.6988 - val_loss: 0.6387 - val_binary_crossentropy: 0.6387 - val_acc: 0.6899\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63871, saving model to model/CNN_Images.h1\n",
      "Epoch 2/80\n",
      "441835/441835 [==============================] - 241s 544us/step - loss: 0.5361 - binary_crossentropy: 0.5361 - acc: 0.7145 - val_loss: 0.7659 - val_binary_crossentropy: 0.7659 - val_acc: 0.6482\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.63871\n",
      "Epoch 3/80\n",
      "441835/441835 [==============================] - 239s 542us/step - loss: 0.5305 - binary_crossentropy: 0.5305 - acc: 0.7189 - val_loss: 0.6911 - val_binary_crossentropy: 0.6911 - val_acc: 0.6475\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.63871\n",
      "Epoch 4/80\n",
      "441835/441835 [==============================] - 240s 544us/step - loss: 0.5257 - binary_crossentropy: 0.5257 - acc: 0.7214 - val_loss: 0.6715 - val_binary_crossentropy: 0.6715 - val_acc: 0.6506\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.63871\n",
      "Epoch 5/80\n",
      "441835/441835 [==============================] - 241s 546us/step - loss: 0.5223 - binary_crossentropy: 0.5223 - acc: 0.7242 - val_loss: 0.7459 - val_binary_crossentropy: 0.7459 - val_acc: 0.5909\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.63871\n",
      "Epoch 6/80\n",
      "441835/441835 [==============================] - 244s 553us/step - loss: 0.5193 - binary_crossentropy: 0.5193 - acc: 0.7261 - val_loss: 0.7212 - val_binary_crossentropy: 0.7212 - val_acc: 0.6408\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.63871\n",
      "Epoch 7/80\n",
      "441835/441835 [==============================] - 244s 552us/step - loss: 0.5170 - binary_crossentropy: 0.5170 - acc: 0.7279 - val_loss: 0.6991 - val_binary_crossentropy: 0.6991 - val_acc: 0.6437\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.63871\n",
      "Epoch 8/80\n",
      "441835/441835 [==============================] - 241s 545us/step - loss: 0.5159 - binary_crossentropy: 0.5159 - acc: 0.7287 - val_loss: 0.6958 - val_binary_crossentropy: 0.6958 - val_acc: 0.6463\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.63871\n",
      "Epoch 9/80\n",
      "441835/441835 [==============================] - 241s 546us/step - loss: 0.5142 - binary_crossentropy: 0.5142 - acc: 0.7299 - val_loss: 0.7058 - val_binary_crossentropy: 0.7058 - val_acc: 0.6388\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.63871\n",
      "Epoch 10/80\n",
      "441835/441835 [==============================] - 244s 553us/step - loss: 0.5127 - binary_crossentropy: 0.5127 - acc: 0.7307 - val_loss: 0.6915 - val_binary_crossentropy: 0.6915 - val_acc: 0.6662\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.63871\n",
      "Epoch 11/80\n",
      "441835/441835 [==============================] - 244s 551us/step - loss: 0.5111 - binary_crossentropy: 0.5111 - acc: 0.7318 - val_loss: 0.6552 - val_binary_crossentropy: 0.6552 - val_acc: 0.6833\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.63871\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"model/CNN_Images.h1\"\n",
    "if not os.path.exists(\"model\"):\n",
    "    os.mkdir(\"model\")\n",
    "\n",
    "model_checkpoint = keras.callbacks.ModelCheckpoint(checkpoint_path, monitor = 'val_loss', \n",
    "                                                   verbose = 1, save_best_only = True, \n",
    "                                                   save_weights_only = False, mode = 'auto', \n",
    "                                                   period = 1)    \n",
    "EPOCHS = 80\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 10)\n",
    "history = CNN.fit(\n",
    "    X_train, Y_train,\n",
    "    epochs = EPOCHS, \n",
    "    validation_split = 0.2,\n",
    "    verbose = 1,\n",
    "    callbacks = [early_stop, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"history\"):\n",
    "    os.mkdir(\"history\")\n",
    "with open('history/CNN_Images.history', 'wb') as file_pi:\n",
    "    pickle.dump(history.history, file_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model with all Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Features for Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all possible features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features will be loaded into X\n",
    "features = feat_all\n",
    "\n",
    "# label into Y\n",
    "label = ['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = build_XY(features, label, train_sig, train_bkg)\n",
    "Y_train = [Y_train[0][:,:2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN = build_CNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test model's prediction $before$ training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 9ms/step\n",
      "[[0.5061053  0.4938947 ]\n",
      " [0.4979751  0.5020249 ]\n",
      " [0.49662575 0.50337416]\n",
      " [0.4896791  0.5103209 ]\n",
      " [0.4931998  0.5068002 ]\n",
      " [0.49717    0.50283   ]\n",
      " [0.49570665 0.5042933 ]\n",
      " [0.49475947 0.50524056]\n",
      " [0.500486   0.49951392]\n",
      " [0.5027973  0.49720272]]\n",
      "[0.6881012320518494, 0.6881012320518494, 0.699999988079071]\n"
     ]
    }
   ],
   "source": [
    "X_batch = [ele[:10] for ele in X_train]\n",
    "Y_batch = [ele[:10] for ele in Y_train]\n",
    "example_result = CNN.predict(x = X_batch)\n",
    "results = CNN.evaluate(x = X_batch, y = Y_batch )\n",
    "print(example_result)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train! (warning: if building CNN, computer tends to get loud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 441835 samples, validate on 110459 samples\n",
      "Epoch 1/80\n",
      "441835/441835 [==============================] - 259s 587us/step - loss: 0.3622 - binary_crossentropy: 0.3622 - acc: 0.8319 - val_loss: 0.3764 - val_binary_crossentropy: 0.3764 - val_acc: 0.8632\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37639, saving model to model/CNN_all.h8\n",
      "Epoch 2/80\n",
      "441835/441835 [==============================] - 259s 586us/step - loss: 0.3330 - binary_crossentropy: 0.3330 - acc: 0.8480 - val_loss: 0.3802 - val_binary_crossentropy: 0.3802 - val_acc: 0.8670\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.37639\n",
      "Epoch 3/80\n",
      "441835/441835 [==============================] - 257s 582us/step - loss: 0.3267 - binary_crossentropy: 0.3267 - acc: 0.8510 - val_loss: 0.3868 - val_binary_crossentropy: 0.3868 - val_acc: 0.8521\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.37639\n",
      "Epoch 4/80\n",
      "441835/441835 [==============================] - 261s 591us/step - loss: 0.3228 - binary_crossentropy: 0.3228 - acc: 0.8528 - val_loss: 0.4577 - val_binary_crossentropy: 0.4577 - val_acc: 0.8179\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.37639\n",
      "Epoch 5/80\n",
      "441835/441835 [==============================] - 260s 588us/step - loss: 0.3195 - binary_crossentropy: 0.3195 - acc: 0.8547 - val_loss: 0.3777 - val_binary_crossentropy: 0.3777 - val_acc: 0.8590\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.37639\n",
      "Epoch 6/80\n",
      "441835/441835 [==============================] - 259s 587us/step - loss: 0.3170 - binary_crossentropy: 0.3170 - acc: 0.8558 - val_loss: 0.4742 - val_binary_crossentropy: 0.4742 - val_acc: 0.7891\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.37639\n",
      "Epoch 7/80\n",
      "441835/441835 [==============================] - 262s 592us/step - loss: 0.3156 - binary_crossentropy: 0.3156 - acc: 0.8562 - val_loss: 0.3995 - val_binary_crossentropy: 0.3995 - val_acc: 0.8485\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.37639\n",
      "Epoch 8/80\n",
      "441835/441835 [==============================] - 257s 581us/step - loss: 0.3140 - binary_crossentropy: 0.3140 - acc: 0.8575 - val_loss: 0.4165 - val_binary_crossentropy: 0.4165 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.37639\n",
      "Epoch 9/80\n",
      "441835/441835 [==============================] - 257s 581us/step - loss: 0.3128 - binary_crossentropy: 0.3128 - acc: 0.8581 - val_loss: 0.4109 - val_binary_crossentropy: 0.4109 - val_acc: 0.8400\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.37639\n",
      "Epoch 10/80\n",
      "441835/441835 [==============================] - 257s 582us/step - loss: 0.3117 - binary_crossentropy: 0.3117 - acc: 0.8583 - val_loss: 0.4032 - val_binary_crossentropy: 0.4032 - val_acc: 0.8429\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.37639\n",
      "Epoch 11/80\n",
      "441835/441835 [==============================] - 258s 585us/step - loss: 0.3108 - binary_crossentropy: 0.3108 - acc: 0.8585 - val_loss: 0.3841 - val_binary_crossentropy: 0.3841 - val_acc: 0.8532\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.37639\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"model/CNN_all.h8\"\n",
    "if not os.path.exists(\"model\"):\n",
    "    os.mkdir(\"model\")\n",
    "\n",
    "model_checkpoint = keras.callbacks.ModelCheckpoint(checkpoint_path, monitor = 'val_loss', \n",
    "                                                   verbose = 1, save_best_only = True, \n",
    "                                                   save_weights_only = False, mode = 'auto', \n",
    "                                                   period = 1)    \n",
    "EPOCHS = 80\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 10)\n",
    "history = CNN.fit(\n",
    "    X_train, Y_train,\n",
    "    epochs = EPOCHS,\n",
    "    validation_split = 0.2,\n",
    "    verbose = 1,\n",
    "    callbacks = [early_stop, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('history/CNN_all.h8.history', 'wb') as file_pi:\n",
    "    pickle.dump(history.history, file_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Features for Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all possible features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jetImages',\n",
       " 'jetMassSD',\n",
       " 'deltaR_subjets',\n",
       " 'jetPt',\n",
       " 'z',\n",
       " 'jetMass',\n",
       " 'jetEta',\n",
       " 'jetPhi',\n",
       " 'tau1_b05',\n",
       " 'tau2_b05',\n",
       " 'tau3_b05',\n",
       " 'tau1_sd_b05',\n",
       " 'tau2_sd_b05',\n",
       " 'tau3_sd_b05',\n",
       " 'tau1_b10',\n",
       " 'tau2_b10',\n",
       " 'tau3_b10',\n",
       " 'tau1_sd_b10',\n",
       " 'tau2_sd_b10',\n",
       " 'tau3_sd_b10',\n",
       " 'tau1_b15',\n",
       " 'tau2_b15',\n",
       " 'tau3_b15',\n",
       " 'tau1_sd_b15',\n",
       " 'tau2_sd_b15',\n",
       " 'tau3_sd_b15',\n",
       " 'tau1_b20',\n",
       " 'tau2_b20',\n",
       " 'tau3_b20',\n",
       " 'tau1_sd_b20',\n",
       " 'tau2_sd_b20',\n",
       " 'tau3_sd_b20',\n",
       " 'chMult',\n",
       " 'neutMult',\n",
       " 'phoMult',\n",
       " 'eleMult',\n",
       " 'muMult',\n",
       " 'beta3',\n",
       " 'beta3_sd',\n",
       " 'tau21',\n",
       " 'jetpull_abs',\n",
       " 'dxy_max',\n",
       " 'dz_max']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_those = [ 'jetImages',\n",
    " 'chMult',\n",
    " 'neutMult',\n",
    " 'phoMult',\n",
    " 'eleMult',\n",
    " 'muMult']\n",
    "# features will be loaded into X\n",
    "features = feat_all[:]\n",
    "for feat in drop_those:\n",
    "    features.remove(feat)\n",
    "# label into Y\n",
    "label = ['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jetMassSD',\n",
       " 'deltaR_subjets',\n",
       " 'jetPt',\n",
       " 'z',\n",
       " 'jetMass',\n",
       " 'jetEta',\n",
       " 'jetPhi',\n",
       " 'tau1_b05',\n",
       " 'tau2_b05',\n",
       " 'tau3_b05',\n",
       " 'tau1_sd_b05',\n",
       " 'tau2_sd_b05',\n",
       " 'tau3_sd_b05',\n",
       " 'tau1_b10',\n",
       " 'tau2_b10',\n",
       " 'tau3_b10',\n",
       " 'tau1_sd_b10',\n",
       " 'tau2_sd_b10',\n",
       " 'tau3_sd_b10',\n",
       " 'tau1_b15',\n",
       " 'tau2_b15',\n",
       " 'tau3_b15',\n",
       " 'tau1_sd_b15',\n",
       " 'tau2_sd_b15',\n",
       " 'tau3_sd_b15',\n",
       " 'tau1_b20',\n",
       " 'tau2_b20',\n",
       " 'tau3_b20',\n",
       " 'tau1_sd_b20',\n",
       " 'tau2_sd_b20',\n",
       " 'tau3_sd_b20',\n",
       " 'beta3',\n",
       " 'beta3_sd',\n",
       " 'tau21',\n",
       " 'jetpull_abs',\n",
       " 'dxy_max',\n",
       " 'dz_max']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, Y_train = build_XY(features,label,data_train)\n",
    "X_train, Y_train = build_XY(features, label, train_sig, train_bkg)\n",
    "Y_train = [Y_train[0][:,:2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN = build_DNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test model's prediction $before$ training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 7ms/step\n",
      "[[0.5486917  0.45130834]\n",
      " [0.6293871  0.37061295]\n",
      " [0.6313691  0.3686309 ]\n",
      " [0.55587584 0.4441242 ]\n",
      " [0.6003828  0.3996172 ]\n",
      " [0.57158464 0.4284154 ]\n",
      " [0.55627143 0.4437286 ]\n",
      " [0.54628944 0.4537106 ]\n",
      " [0.61265916 0.38734087]\n",
      " [0.5552818  0.44471818]]\n",
      "[0.8724324107170105, 0.8724324107170105, 0.0]\n"
     ]
    }
   ],
   "source": [
    "X_batch = [ele[:10] for ele in X_train]\n",
    "Y_batch = [ele[:10] for ele in Y_train]\n",
    "example_result = CNN.predict(x = X_batch)\n",
    "results = CNN.evaluate(x = X_batch, y = Y_batch )\n",
    "print(example_result)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train! (warning: if building CNN, computer tends to get loud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 441835 samples, validate on 110459 samples\n",
      "Epoch 1/60\n",
      "441835/441835 [==============================] - 39s 87us/step - loss: 0.3845 - binary_crossentropy: 0.3845 - acc: 0.8201 - val_loss: 0.3562 - val_binary_crossentropy: 0.3562 - val_acc: 0.8847\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.35617, saving model to model/DNN_noflavour.h1\n",
      "Epoch 2/60\n",
      "441835/441835 [==============================] - 38s 87us/step - loss: 0.3470 - binary_crossentropy: 0.3470 - acc: 0.8407 - val_loss: 0.4804 - val_binary_crossentropy: 0.4804 - val_acc: 0.8118\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.35617\n",
      "Epoch 3/60\n",
      "441835/441835 [==============================] - 39s 87us/step - loss: 0.3411 - binary_crossentropy: 0.3411 - acc: 0.8439 - val_loss: 0.5720 - val_binary_crossentropy: 0.5720 - val_acc: 0.7322\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35617\n",
      "Epoch 4/60\n",
      "441835/441835 [==============================] - 38s 86us/step - loss: 0.3409 - binary_crossentropy: 0.3409 - acc: 0.8450 - val_loss: 0.4418 - val_binary_crossentropy: 0.4418 - val_acc: 0.8411\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35617\n",
      "Epoch 5/60\n",
      "441835/441835 [==============================] - 38s 86us/step - loss: 0.3415 - binary_crossentropy: 0.3415 - acc: 0.8449 - val_loss: 0.3629 - val_binary_crossentropy: 0.3629 - val_acc: 0.8800\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35617\n",
      "Epoch 6/60\n",
      "441835/441835 [==============================] - 38s 85us/step - loss: 0.3425 - binary_crossentropy: 0.3425 - acc: 0.8448 - val_loss: 0.4040 - val_binary_crossentropy: 0.4040 - val_acc: 0.8474\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35617\n",
      "Epoch 7/60\n",
      "441835/441835 [==============================] - 38s 86us/step - loss: 0.3434 - binary_crossentropy: 0.3434 - acc: 0.8451 - val_loss: 0.4173 - val_binary_crossentropy: 0.4173 - val_acc: 0.8462\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35617\n",
      "Epoch 8/60\n",
      "441835/441835 [==============================] - 39s 87us/step - loss: 0.3442 - binary_crossentropy: 0.3442 - acc: 0.8452 - val_loss: 0.4880 - val_binary_crossentropy: 0.4880 - val_acc: 0.8254\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35617\n",
      "Epoch 9/60\n",
      "441835/441835 [==============================] - 38s 86us/step - loss: 0.3455 - binary_crossentropy: 0.3455 - acc: 0.8453 - val_loss: 0.4378 - val_binary_crossentropy: 0.4378 - val_acc: 0.8299\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.35617\n",
      "Epoch 10/60\n",
      "441835/441835 [==============================] - 38s 87us/step - loss: 0.3465 - binary_crossentropy: 0.3465 - acc: 0.8451 - val_loss: 0.4537 - val_binary_crossentropy: 0.4537 - val_acc: 0.8179\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.35617\n",
      "Epoch 11/60\n",
      "441835/441835 [==============================] - 38s 87us/step - loss: 0.3467 - binary_crossentropy: 0.3467 - acc: 0.8454 - val_loss: 0.4806 - val_binary_crossentropy: 0.4806 - val_acc: 0.8045\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.35617\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"model/DNN_noflavour.h1\"\n",
    "if not os.path.exists(\"model\"):\n",
    "    os.mkdir(\"model\")\n",
    "\n",
    "model_checkpoint = keras.callbacks.ModelCheckpoint(checkpoint_path, monitor = 'val_loss', \n",
    "                                                   verbose = 1, save_best_only = True, \n",
    "                                                   save_weights_only = False, mode = 'auto', \n",
    "                                                   period = 1)    \n",
    "EPOCHS = 60\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 10)\n",
    "history = CNN.fit(\n",
    "    X_train, Y_train,\n",
    "    epochs = EPOCHS,\n",
    "    validation_split = 0.2,\n",
    "    verbose = 1,\n",
    "    callbacks = [early_stop, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('history/DNN_noflavour.h1.history', 'wb') as file_pi:\n",
    "    pickle.dump(history.history, file_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
